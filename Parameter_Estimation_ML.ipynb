{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b15a13a",
   "metadata": {},
   "source": [
    "# Using ML for paramter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f91eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sl\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\"\"\"Loading the Data\"\"\"\n",
    "\n",
    "dir       = 'data/'\n",
    "file_name = 'grid1_zheb51fo.xlsx'\n",
    "UX1       = pd.read_excel(dir+file_name, sheet_name='UX1_Index')\n",
    "UX2       = pd.read_excel(dir+file_name, sheet_name='UX2_Index')\n",
    "UX1       = UX1.set_index('Date')\n",
    "UX2       = UX2.set_index('Date')\n",
    "UX1.dropna(subset = [\"PX_LAST\"], inplace=True)   #Getting rid of NaN values\n",
    "UX2.dropna(subset = [\"PX_LAST\"], inplace=True)\n",
    "UX1.sort_index(inplace=True)\n",
    "UX2.sort_index(inplace=True)\n",
    "dataset = np.array(UX1.PX_LAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693af0a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Specifying the Input & Output (Labels)\"\"\"\n",
    "\n",
    "n=0     #Looking at n previous days to estimate paramteres + today's volatility\n",
    "X= [[dataset[j] for j in range(i, i+n+1)] for i in range(len(dataset) - n - 1)]\n",
    "Y= [dataset[i+n+1] for i in range(len(dataset) - n -1)]\n",
    "\n",
    "# Putting more emphasis on today's data:f\n",
    "\n",
    "# emphasis = 1    #repearing today's value emphasis times\n",
    "# for i in range(len(X)):\n",
    "#     for j in range(emphasis-1):\n",
    "#         X[i].append(X[i][-1])        \n",
    "        \n",
    "# print(X[:10])\n",
    "# print(\"\\n\")\n",
    "# print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a680e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting Data into Train and Test set\"\"\"\n",
    "\n",
    "m_training= 2000\n",
    "m_test= 1000\n",
    "\n",
    "X_training= X[:m_training]\n",
    "Y_training= Y[:m_training]   \n",
    "X_test= X[m_training:m_training+m_test]\n",
    "Y_test= Y[m_training:m_training+m_test]\n",
    "\n",
    "if n==0:\n",
    "    X_training = np.ravel(X_training)\n",
    "    X_test = np.ravel(X_test)\n",
    "    Y_training = np.array(Y_training)\n",
    "    Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea0fd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building the Hypothesis\"\"\"\n",
    "\n",
    "def heston_pde_milstein(V0, k, theta, rho, sigma):\n",
    "    WT  = np.sqrt( 1 ) * np.random.multivariate_normal(np.array([0, 0]), np.array([[1, rho], [rho, 1]]))[1]\n",
    "    V1 = np.abs(V0+ k * (theta - V0) * 1 + sigma * np.sqrt(V0) * WT + .25 * sigma**2 * (WT**2 - 1))\n",
    "    return V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae600854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building the Loss Function\"\"\"\n",
    "\n",
    "#The difference between real label and the predicted one to the power of 2\n",
    "#l = (heston_pde_milstein(X_training[i], r, k, theta, rho, sigma) - Y_training[i])**2\n",
    "\n",
    "m=len(X_training)  #Training set size\n",
    "# k: x[0], theta:x[1], rho:x[2], sigma:x[3]\n",
    "\n",
    "def Ls(X):\n",
    "    def heston_inner_func(i):   #calculates the predicted lable for each training sample\n",
    "        WT  = np.random.multivariate_normal(np.array([0, 0]), np.array([[1, X[2]], [X[2], 1]]))[1]\n",
    "        V1 =  np.abs(X_training[i] + X[0] * (X[1] - X_training[i]) * 1 + \n",
    "                     X[3] * np.sqrt(X_training[i]) * WT + .25 * X[3]**2 * (WT**2 - 1))\n",
    "        return V1\n",
    "    Ls = (1/m) * np.sum(np.array([(heston_inner_func(i) - Y_training[i])**2 for i in range(m)]))\n",
    "    return Ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc8ff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Re_investigating the Loss Function'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Re_investigating the Loss Function\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5cf629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-7e69c1efd057>:11: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  WT  = np.random.multivariate_normal(np.array([0, 0]), np.array([[1, X[2]], [X[2], 1]]))[1]\n",
      "<ipython-input-5-7e69c1efd057>:11: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  WT  = np.random.multivariate_normal(np.array([0, 0]), np.array([[1, X[2]], [X[2], 1]]))[1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ERM: Empirical Risk Minimization\"\"\"\n",
    "\n",
    "result = minimize(Ls, (1,1,1,1))  #initial values should be given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ecd2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paramteres: [1.00000079 0.99997342 1.00000365 1.00003907]\n",
      "Minimum Ls: 258.4466653462006\n"
     ]
    }
   ],
   "source": [
    "best_params = result.x\n",
    "print(\"Best Paramteres:\", best_params)\n",
    "print(\"Minimum Ls:\", result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba16ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are we trapped inside a loval minima?!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371890d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360.3667427139515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-7abb1314c351>:4: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  WT  = np.sqrt( 1 ) * np.random.multivariate_normal(np.array([0, 0]), np.array([[1, rho], [rho, 1]]))[1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluating the model on test Set\"\"\"\n",
    "\n",
    "k, theta, rho, sigma = best_params[0], best_params[1], best_params[2], best_params[3]\n",
    "Y_pred = np.array([heston_pde_milstein(X_test[i], k, theta, rho, sigma) for i in range(len(X_test))])\n",
    "Y_pred = np.ravel(Y_pred)\n",
    "\n",
    "#True Error\n",
    "Ld =  (1/len(Y_test)) * np.sum((Y_pred - Y_test)**2)\n",
    "print(Ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = np.array([heston_pde_milstein(X_training[i], k, theta, rho, sigma) for i in range(len(X_training))])\n",
    "L =  (1/m) * np.sum((Y_pred_train - Y_training)**2)\n",
    "print(L)\n",
    "\n",
    "print(Y_pred_train[:50])\n",
    "print(Y_training[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645d01a",
   "metadata": {},
   "source": [
    "## Using Neural Network for the Whole task: A Non-Physical Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c06c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's, just for a momemnt, Ignore any pre_knowledge about the subject and see what happens\n",
    "\n",
    "m_training= 2000\n",
    "m_test= 1000\n",
    "\n",
    "X_training=X[:m_training]\n",
    "Y_training=Y[:m_training]   \n",
    "X_test=X[m_training:m_training+m_test]\n",
    "Y_test=Y[m_training:m_training+m_test]\n",
    "\n",
    "if n==0:\n",
    "    X_training= np.ravel(X_training)  \n",
    "    X_test    = np.ravel(X_test)\n",
    "    \n",
    "#Let's scale the Volatilities:\n",
    "M = 100    #M times magnification\n",
    "\n",
    "X_training= np.array(X_training) * M\n",
    "Y_training= np.array(Y_training) * M\n",
    "X_test= np.array(X_test) * M\n",
    "Y_test= np.array(Y_test) * M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_R = MLPRegressor(hidden_layer_sizes=(20, 20, 20, 20), max_iter=1000, \n",
    "#                     alpha=1e-4, solver='adam', momentum=0.9,\n",
    "#                     activation='relu', tol=1e-4, learning_rate_init=0.001)\n",
    "\n",
    "# NN_R.fit(X_training, Y_training)\n",
    "# Y_pred = NN_R.predict(X_test)\n",
    "# # print(\"Predicted Volatilty:\\n\", Y_pred[100:120])\n",
    "# # print(\"\\nTrue Volatility:\\n\",     Y_test[100:120])\n",
    "# print(\"\\nThe score:\", NN_R.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at this point we don't know if we are getting a good score just beacuse of the similarity between volatlities or\n",
    "#the model is working properly, we should try to figure out a way to find this!\n",
    "#let's plot the graphs and see what's going on\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(13,6))\n",
    "plt.scatter([t for t in range(len(Y_test))], Y_test, alpha= 0.6,color='gold', marker='.', label='Y_test')\n",
    "plt.scatter([t for t in range(len(Y_test))], Y_pred, alpha= 0.6,color='b', marker='.', label='Y_pred')\n",
    "plt.scatter([t for t in range(len(Y_test))], \n",
    "            [X_test[i][-1] for i in range(len(Y_test))],alpha= 0.6, color='r', marker='.', label='X_test')\n",
    "# plt.xlim(550,600)\n",
    "# plt.ylim(3000,5000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d521c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems that the result is merely a translation of today's volalitilies not the predictions for tomorrow!\n",
    "#but let's measure this preciesly\n",
    "\n",
    "# Distance_from_tomorrow    = np.sum(np.abs(Y_pred - Y_test))\n",
    "# Distance_from_today = np.sum(np.abs(\n",
    "#             np.array(Y_pred - [X_test[i][-1] for i in range(len(list(Y_test)))])))\n",
    "\n",
    "# plt.figure(figsize=(13,6))\n",
    "# plt.scatter([t for t in range(len(Y_pred))], Distance_from_today, marker = '+', label = 'Distance to Today')\n",
    "# plt.scatter([t for t in range(len(Y_pred))], Distance_from_tomorrow, marker = '+', label = 'Distance to Tomorrow')\n",
    "# # plt.xlim(100,140)\n",
    "# # plt.ylim(-0.5,1)\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "# print(\"Distance to Today:\\n\", np.sum(Distance_from_today))\n",
    "# print(\"\\nDistance to Tommorow:\\n\", np.sum(Distance_from_tomorrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d101f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Investigate the behaviour of Distance w.r to n and emphasis(Looking at n previous data)\n",
    "#Just Run the fist Cell before running this cell\n",
    "\n",
    "D_today = []  #The list to save all values\n",
    "D_tomorrow = []\n",
    "\n",
    "for emphasis in [3]:\n",
    "    Distance_from_today_list    = []\n",
    "    Distance_from_tomorrow_list = []\n",
    "    \n",
    "    for n in range(0,100,5):\n",
    "        X= [[dataset[j] for j in range(i, i+n+1)] for i in range(len(dataset) - n - 1)]\n",
    "        Y= [dataset[i+n+1] for i in range(len(dataset) - n -1)]\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            for j in range(emphasis-1):\n",
    "                X[i].append(X[i][-1])    \n",
    "                \n",
    "        m_training = 2000\n",
    "        m_test     = 1000\n",
    "        X_training=X[:m_training]\n",
    "        Y_training=Y[:m_training]   \n",
    "        X_test=X[m_training:m_training+m_test]\n",
    "        Y_test=Y[m_training:m_training+m_test]\n",
    "                 \n",
    "        M = 10    #M times magnification\n",
    "        X_training= np.array(X_training) * M\n",
    "        Y_training= np.array(Y_training) * M\n",
    "        X_test= np.array(X_test) * M\n",
    "        Y_test= np.array(Y_test) * M\n",
    "    \n",
    "        NN_R = MLPRegressor(hidden_layer_sizes=(20,20,20), max_iter=1000, \n",
    "                        alpha=1e-4, solver='adam', momentum=0.9,\n",
    "                        activation='relu', tol=1e-4, learning_rate_init=0.01)\n",
    "        NN_R.fit(X_training, Y_training)\n",
    "        Y_pred = NN_R.predict(X_test)\n",
    "    \n",
    "        Distance_from_tomorrow    = np.sum(np.abs(Y_pred - Y_test))\n",
    "        Distance_from_today = np.sum(np.abs(\n",
    "            np.array(Y_pred - [X_test[i][-1] for i in range(len(list(Y_test)))])))\n",
    "        Distance_from_today_list.append(Distance_from_today)\n",
    "        Distance_from_tomorrow_list.append(Distance_from_tomorrow)\n",
    "\n",
    "    D_today.append(Distance_from_today_list)\n",
    "    D_tomorrow.append(Distance_from_tomorrow_list)\n",
    "    \n",
    "    #Normalization:\n",
    "    D_today= np.array(D_today)/(max(np.array(D_today).max(),np.array(D_tomorrow).max()))\n",
    "    D_tomorrow= np.array(D_tomorrow)/(max(np.array(D_today).max(), np.array(D_tomorrow).max()))\n",
    "    \n",
    "    # Plotting the Result of previous part\n",
    "plt.figure(figsize=(13,7))\n",
    "# plt.plot([t for t in range(L)], Distance_from_today_list, )\n",
    "plt.plot([t for t in range(len(D_tomorrow[0]))], D_tomorrow[0],\n",
    "            marker=\"o\", label = 'tom')\n",
    "plt.plot([t for t in range(len(D_today[0]))],    D_today[0],\n",
    "            marker=\"o\", label = 'tod')\n",
    "# plt.scatter([t for t in range(len(D_tomorrow[0]))], D_tomorrow[1],\n",
    "#             marker=\"+\",color='b', label = 'Emphasis =3')\n",
    "# plt.scatter([t for t in range(len(D_tomorrow[0]))], D_tomorrow[2],\n",
    "#             marker=\"+\",color='r', label = 'Emphasis =5')\n",
    "# plt.scatter([t for t in range(len(D_tomorrow[0]))], D_tomorrow[3],\n",
    "#             marker=\"+\",color='0', label = 'Emphasis =10')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Distance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection, Grid Search CV\n",
    "NN_R = MLPRegressor(max_iter=1000, alpha=1e-4, solver='adam', momentum=0.9,\n",
    "                    activation='relu', tol=1e-4, learning_rate_init=0.001)\n",
    "parameters = {'hidden_layer_sizes': [(20),(20,20),(20,20,20),(20,20,20,20),(20,20,20,20,20)]}\n",
    "\n",
    "gsc = GridSearchCV(NN_R, parameters, cv=5)\n",
    "\n",
    "gsc.fit(X_training, Y_training)\n",
    "\n",
    "print(\"Best parameters set found:\")\n",
    "print(gsc.best_params_)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Score with best parameters:\")\n",
    "print(gsc.best_score_)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"All scores on the grid:\")\n",
    "print(gsc.cv_results_.get('split0_test_score'))\n",
    "print(gsc.cv_results_.get('split1_test_score'))\n",
    "print(gsc.cv_results_.get('split2_test_score'))\n",
    "print(gsc.cv_results_.get('split3_test_score'))\n",
    "print(gsc.cv_results_.get('split4_test_score'))\n",
    "print(gsc.cv_results_.get('split5_test_score'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
