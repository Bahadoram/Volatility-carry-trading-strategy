{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b15a13a",
   "metadata": {},
   "source": [
    "# Using ML for paramter estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f91eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sl\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\"\"\"Loading the Data\"\"\"\n",
    "\n",
    "dir       = 'data/'\n",
    "file_name = 'grid1_zheb51fo.xlsx'\n",
    "UX1       = pd.read_excel(dir+file_name, sheet_name='UX1_Index')\n",
    "UX2       = pd.read_excel(dir+file_name, sheet_name='UX2_Index')\n",
    "UX1       = UX1.set_index('Date')\n",
    "UX2       = UX2.set_index('Date')\n",
    "UX1.sort_index(inplace=True)\n",
    "UX2.sort_index(inplace=True)\n",
    "dataset = np.array(UX1.PX_LAST)\n",
    "dataset = dataset[4:]              #we drop 4 data for simpler illustratoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693af0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Specifying the Input & Output (Labels)\"\"\"\n",
    "\n",
    "n=0     #Looking at n previous days to estimate paramteres\n",
    "X=np.array([[dataset[j] for j in range(i, i+n+1)] for i in range(len(dataset) - n)])\n",
    "Y=np.array([dataset[i+n] for i in range(len(dataset) - n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a680e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting Data into Train and Test set\"\"\"\n",
    "\n",
    "m_training= 2000\n",
    "m_test= 1000\n",
    "\n",
    "X_training=X[ :m_training  ]\n",
    "Y_training=Y[1:m_training+1]   #The labels are shifted 1 to the right since they are tomorrow's value\n",
    "\n",
    "X_test=X[m_training:-1]\n",
    "Y_test=Y[m_training+1:]\n",
    "\n",
    "#for the case of n=0\n",
    "X_training= np.ravel(X_training)  \n",
    "X_test    = np.ravel(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4836c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing NaN values:\n",
    "nan_array = np.isnan(X_test)\n",
    "not_nan_array = ~ nan_array\n",
    "X_test = X_test[not_nan_array]\n",
    "\n",
    "nan_array = np.isnan(Y_test)\n",
    "not_nan_array = ~ nan_array\n",
    "Y_test = Y_test[not_nan_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea0fd5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building the Hypothesis\"\"\"\n",
    "\n",
    "def heston_pde_milstein(V0, k, theta, rho, sigma):\n",
    "    WT  = np.sqrt( 1 ) * np.random.multivariate_normal(np.array([0, 0]), np.array([[1, rho], [rho, 1]]))[1]\n",
    "    V1 = np.abs(V0+ k * (theta - V0) * 1 + sigma * np.sqrt(V0) * WT + .25 * sigma**2 * (WT**2 - 1))\n",
    "    return V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae600854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building the Loss Function\"\"\"\n",
    "\n",
    "#The difference between real label and the predicted one to the power of 2\n",
    "#l = (heston_pde_milstein(X_training[i], r, k, theta, rho, sigma) - Y_training[i])**2\n",
    "\n",
    "m=len(X_training)  #Training set size\n",
    "# k: x[0], theta:x[1], rho:x[2], sigma:x[3]\n",
    "\n",
    "def Ls(X):\n",
    "    def heston_inner_func(i):   #calculates the predicted lable for each training sample\n",
    "        WT  = np.random.multivariate_normal(np.array([0, 0]), np.array([[1, X[2]], [X[2], 1]]))[1]\n",
    "        V1 =  np.abs(X_training[i] + X[0] * (X[0] - X_training[i]) * 1 + \n",
    "                     X[3] * np.sqrt(X_training[i]) * WT + .25 * X[3]**2 * (WT**2 - 1))\n",
    "        return V1\n",
    "    Ls = (1/m) * np.sum(np.array([(heston_inner_func(i) - Y_training[i])**2 for i in range(m)]))\n",
    "    return Ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5cf629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ERM: Empirical Risk Minimization\"\"\"\n",
    "\n",
    "result = minimize(Ls, (0,0,0,0))  #initial values should be given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ecd2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paramteres: [ 4.48473614e-07 -2.40657956e-08  1.10410470e-07 -5.16934758e-08]\n",
      "Minimum Ls: 0.8302061008517941\n"
     ]
    }
   ],
   "source": [
    "best_params = result.x\n",
    "print(\"Best Paramteres:\", best_params)\n",
    "print(\"Minimum Ls:\", result.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "371890d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1170046816827566\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluating the model on test Set\"\"\"\n",
    "\n",
    "k, theta, rho, sigma = best_params[0], best_params[1], best_params[2], best_params[3]\n",
    "Y_pred = np.array([heston_pde_milstein(X_test[i], k, theta, rho, sigma) for i in range(len(X_test))])\n",
    "Y_pred = np.ravel(Y_pred)\n",
    "\n",
    "#True Error\n",
    "Ld =  (1/len(Y_test)) * np.sum((Y_pred - Y_test)**2)\n",
    "print(Ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4904a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8302060864968489\n",
      "[22.54999034 22.24999017 22.79998963 22.49999012 21.99999013 22.54998993\n",
      " 21.54999078 21.39999014 22.64998955 24.74998872 24.34998832 24.54998878\n",
      " 23.74998943 23.99998896 24.64998923 24.24998922 23.44998958 23.54998998\n",
      " 25.64998851 25.84998797 26.19998881 25.64998865 25.54998844 24.94998887\n",
      " 24.89998885 23.5999894  22.74998992 21.79998991 21.34999013 20.94999025\n",
      " 21.64999015 21.09999049 20.94999024 20.39999101 23.04998982 22.79998947\n",
      " 22.59998967 22.49998972 21.59999083 21.2499907  21.24999033 21.3999904\n",
      " 21.59999065 21.44999058 21.49999051 20.89999049 20.09999126 19.7999913\n",
      " 20.19999111 19.89999109]\n",
      "[22.25 22.8  22.5  22.   22.55 21.55 21.4  22.65 24.75 24.35 24.55 23.75\n",
      " 24.   24.65 24.25 23.45 23.55 25.65 25.85 26.2  25.65 25.55 24.95 24.9\n",
      " 23.6  22.75 21.8  21.35 20.95 21.65 21.1  20.95 20.4  23.05 22.8  22.6\n",
      " 22.5  21.6  21.25 21.25 21.4  21.6  21.45 21.5  20.9  20.1  19.8  20.2\n",
      " 19.9  19.45]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = np.array([heston_pde_milstein(X_training[i], k, theta, rho, sigma) for i in range(len(X_training))])\n",
    "L =  (1/m) * np.sum((Y_pred_train - Y_training)**2)\n",
    "print(L)\n",
    "\n",
    "print(Y_pred_train[:50])\n",
    "print(Y_training[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Using Neural Network for Whole task: A Non-Physical Experiment\"\"\"\n",
    "\n",
    "#Let's, just for a momemnt, Ignore any pre_knowledge about the subject and see what happens!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
